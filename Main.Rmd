---
title: "FinEcon2_PS2"
author: "Bjorn Jivung"
date: "2026-01-17"
output: html_document
---


## Step 0: Global knit options and a clean session

This chunk sets consistent R Markdown behavior and wipes the workspace so every run starts from a clean slate.

```{r}

# Knit behavior (controls how code + outputs appear in the rendered document)

knitr::opts_chunk$set(
echo    = TRUE,   # show code in the knitted output
warning = TRUE,   # keep warnings visible while developing
message = TRUE    # show messages (e.g., package startup); can switch to FALSE later
)

# Start from a clean workspace

rm(list = ls())     # remove all objects in the Global Environment
gc()                # garbage collection: frees memory

# Reproducibility / print settings

options(stringsAsFactors = FALSE)  # consistent behavior across R versions
options(scipen = 999)              # reduce scientific notation in printed numbers
set.seed(123)                      # reproducibility for anything stochastic

```


##Step 0.1: Install and load the packages for this project

This chunk installs (if needed) and loads the libraries used in PS2 (event study, Bacon-Goodman decomposition, and imputation-style DID).

```{r}

# Make installs non-interactive and consistent

options(repos = c(CRAN = "https://cloud.r-project.org"))

# Package list for this project (based on PS2 requirements)

pkgs <- c(

# Data import and wrangling

"data.table", "dplyr", "tidyr", "readr", "haven",
"stringr", "janitor", "lubridate",

# Visualization

"ggplot2", "scales",

# Fixed effects, inference, and output

"fixest", "broom", "modelsummary",
"lmtest", "sandwich", "clubSandwich",

# Difference-in-differences tools

"bacondecomp",      # Goodman-Bacon decomposition
"did",              # Callaway & Sant‚ÄôAnna
"didimputation",    # Borusyak et al. imputation estimator

# Utilities

"here", "rstudioapi"
)

# Install any packages that are missing

missing_pkgs <- pkgs[!pkgs %in% rownames(installed.packages())]

if (length(missing_pkgs) > 0) {
install.packages(missing_pkgs, type = "binary")
}

# Load all packages

invisible(lapply(pkgs, library, character.only = TRUE))

# Quick version check for key econometrics packages

cat("\nKey package versions:\n")
cat("fixest:        ", as.character(packageVersion("fixest")), "\n")
cat("bacondecomp:   ", as.character(packageVersion("bacondecomp")), "\n")
cat("did:           ", as.character(packageVersion("did")), "\n")
cat("didimputation: ", as.character(packageVersion("didimputation")), "\n")

```


## Step 0.2: Set the project root and define standard folder paths

This chunk defines a reliable project root directory and creates/records the key folders used throughout the project so all file paths stay consistent and relative.

```{r}

# Project root and path management (single source of truth)

# Use the active RStudio Project path if available; otherwise fall back to the current working directory

PROJECT_DIR <- NA_character_

if (rstudioapi::isAvailable()) {
proj <- tryCatch(rstudioapi::getActiveProject(), error = function(e) NA_character_)
if (!is.na(proj) && nzchar(proj)) PROJECT_DIR <- proj
}

if (is.na(PROJECT_DIR) || !nzchar(PROJECT_DIR)) {
PROJECT_DIR <- getwd()
}

PROJECT_DIR <- normalizePath(PROJECT_DIR, winslash = "/")
cat("Project directory:\n", PROJECT_DIR, "\n")

# Define standard folders (edit/add as needed later)

DATA_RAW_DIR   <- file.path(PROJECT_DIR, "data_raw")
DATA_CLEAN_DIR <- file.path(PROJECT_DIR, "data_clean")
R_DIR          <- file.path(PROJECT_DIR, "R")
ANALYSIS_DIR   <- file.path(PROJECT_DIR, "analysis")
OUTPUT_DIR     <- file.path(PROJECT_DIR, "output")
FIG_DIR        <- file.path(OUTPUT_DIR, "figures")
TAB_DIR        <- file.path(OUTPUT_DIR, "tables")

# Create folders safely (no warnings if they already exist)

dir.create(DATA_RAW_DIR,   recursive = TRUE, showWarnings = FALSE)
dir.create(DATA_CLEAN_DIR, recursive = TRUE, showWarnings = FALSE)
dir.create(R_DIR,          recursive = TRUE, showWarnings = FALSE)
dir.create(ANALYSIS_DIR,   recursive = TRUE, showWarnings = FALSE)
dir.create(FIG_DIR,        recursive = TRUE, showWarnings = FALSE)
dir.create(TAB_DIR,        recursive = TRUE, showWarnings = FALSE)

# Quick sanity checks: list the main project folders

cat("\nFolder paths:\n")
cat("DATA_RAW_DIR:   ", DATA_RAW_DIR, "\n")
cat("DATA_CLEAN_DIR: ", DATA_CLEAN_DIR, "\n")
cat("R_DIR:          ", R_DIR, "\n")
cat("ANALYSIS_DIR:   ", ANALYSIS_DIR, "\n")
cat("OUTPUT_DIR:     ", OUTPUT_DIR, "\n")
cat("FIG_DIR:        ", FIG_DIR, "\n")
cat("TAB_DIR:        ", TAB_DIR, "\n")

# Confirm they exist

stopifnot(
dir.exists(DATA_RAW_DIR),
dir.exists(DATA_CLEAN_DIR),
dir.exists(R_DIR),
dir.exists(ANALYSIS_DIR),
dir.exists(OUTPUT_DIR),
dir.exists(FIG_DIR),
dir.exists(TAB_DIR)
)

```


## Step 0.3: Git diagnostics and repository status checks

This chunk verifies that Git is installed, confirms the project is a Git repository, and prints key repo info (branch + remotes).

```{r}

# Git diagnostics (confirm Git works and this folder is a repo)

git_version <- tryCatch(system("git --version", intern = TRUE), error = function(e) character(0))

if (length(git_version) == 0) {
cat("Git status: NOT FOUND\n")
cat("Fix: Install Git and restart RStudio so Git is available in your PATH.\n\n")
} else {
cat("Git status: FOUND\n")
cat("Git version:\n", paste(git_version, collapse = "\n"), "\n\n", sep = "")
}

# Check whether we are inside a Git repository

git_toplevel <- tryCatch(system("git rev-parse --show-toplevel", intern = TRUE), error = function(e) character(0))

if (length(git_toplevel) == 0) {
cat("Repository status: NOT a Git repository (no .git detected)\n")
cat("If needed, initialize from the project folder in a Terminal:\n")
cat("  git init\n  git branch -M main\n\n")
} else {
cat("Repository status: Git repository detected\n")
cat("Repo top-level folder:\n", paste(git_toplevel, collapse = "\n"), "\n\n", sep = "")

# Current branch

branch <- tryCatch(system("git branch --show-current", intern = TRUE), error = function(e) character(0))
if (length(branch) > 0) cat("Current branch: ", branch, "\n\n", sep = "")

# Remote(s)

remotes <- tryCatch(system("git remote -v", intern = TRUE), error = function(e) character(0))
if (length(remotes) == 0) {
cat("Remotes: none configured yet\n")
cat("If you created a GitHub repo, add it like:\n")
cat("  git remote add origin <YOUR_GITHUB_URL>\n\n")
} else {
cat("Remotes:\n", paste(remotes, collapse = "\n"), "\n\n", sep = "")
}

# Quick status snapshot

status <- tryCatch(system("git status -sb", intern = TRUE), error = function(e) character(0))
if (length(status) > 0) cat("git status (short):\n", paste(status, collapse = "\n"), "\n", sep = "")
}

```


## Step 0.4: Define key file names and centralize data paths

This chunk defines the main data file names in one place so that all later code refers to these objects rather than hard-coding file paths.

```{r}

# Centralized file names (edit once here, use everywhere else)

# Main dataset (raw and cleaned versions)

MAIN_DATA_RAW   <- file.path(DATA_RAW_DIR,   "main_dataset_raw.csv")
MAIN_DATA_CLEAN <- file.path(DATA_CLEAN_DIR, "main_dataset_clean.csv")

# Optional: auxiliary files you may add later

FLOOD_DATA_RAW   <- file.path(DATA_RAW_DIR,   "flood_events_raw.csv")
FLOOD_DATA_CLEAN <- file.path(DATA_CLEAN_DIR, "flood_events_clean.csv")

# Print for sanity check

cat("Key data file paths:\n")
cat("MAIN_DATA_RAW:   ", MAIN_DATA_RAW, "\n")
cat("MAIN_DATA_CLEAN: ", MAIN_DATA_CLEAN, "\n")
cat("FLOOD_DATA_RAW:  ", FLOOD_DATA_RAW, "\n")
cat("FLOOD_DATA_CLEAN:", FLOOD_DATA_CLEAN, "\n")

# Existence checks (non-fatal for now, since data may not be added yet)

if (!file.exists(MAIN_DATA_RAW)) {
cat("\nNOTE: MAIN_DATA_RAW does not exist yet (expected if data not added).\n")
}
if (!file.exists(FLOOD_DATA_RAW)) {
cat("NOTE: FLOOD_DATA_RAW does not exist yet (expected if data not added).\n")
}

```


## Step 0.5: Load the raw dataset and run basic sanity checks

This chunk loads the raw Stata dataset and prints basic information (size, variables, structure).

```{r}

# Load the raw PS2 dataset

PS2_DATA_RAW <- file.path(DATA_RAW_DIR, "PS2_data.dta")

cat("Attempting to load data from:\n", PS2_DATA_RAW, "\n")

if (!file.exists(PS2_DATA_RAW)) {
stop("Data file not found at the specified path. Check DATA_RAW_DIR and filename.")
}

ps2_raw <- haven::read_dta(PS2_DATA_RAW)

# Basic structure checks

cat("\nDataset loaded successfully.\n")
cat("Dimensions (rows, columns):\n")
print(dim(ps2_raw))

cat("\nVariable names:\n")
print(names(ps2_raw))

cat("\nFirst 10 rows:\n")
print(head(ps2_raw, 10))

# Quick panel sanity checks

cat("\nUnique communities (id_num): ", length(unique(ps2_raw$id_num)), "\n")
cat("Unique years: ", length(unique(ps2_raw$year)), "\n")

# Check for duplicate id-year pairs (should generally be none)

dup_check <- ps2_raw %>%
count(id_num, year) %>%
filter(n > 1)

if (nrow(dup_check) == 0) {
cat("\nNo duplicate (id_num, year) pairs found.\n")
} else {
cat("\nWARNING: Duplicate (id_num, year) pairs detected:\n")
print(dup_check)
}

# Check how hityear is coded

cat("\nSummary of hityear variable:\n")
print(table(ps2_raw$hityear, useNA = "ifany"))

# Check how many floods per community (should be at most once by assignment design)

floods_per_id <- ps2_raw %>%
group_by(id_num) %>%
summarise(total_floods = sum(hityear == 1, na.rm = TRUE))

cat("\nDistribution of number of floods per community:\n")
print(table(floods_per_id$total_floods))

```


## Part b) Event-study estimation around flood timing

Part (b) is completed by constructing event time relative to each community‚Äôs flood year, estimating a TWFE event-study regression with fixed effects, and plotting the coefficients with confidence intervals. Interpretation focuses on dynamics before and after the flood and on appropriate clustering choices.

## Step B.1: Create a clean working dataset and confirm basic structure

This chunk creates a working copy of the raw data and enforces consistent variable types needed for the event-time construction and fixed-effects regressions.

```{r}

# Create a working copy (keeps the raw object untouched)

ps2 <- ps2_raw

# Enforce consistent types (fixest prefers clean integer / factor structure)

ps2 <- ps2 %>%
mutate(
state       = as.character(state),
id_num      = as.integer(id_num),
year        = as.integer(year),
hityear     = as.integer(hityear),
ln_policies = as.numeric(ln_policies)
)

# Basic structure checks (lightweight, intended to fail fast if something is off)

cat("Rows, columns:\n")
print(dim(ps2))

cat("\nYear range:\n")
print(range(ps2$year, na.rm = TRUE))

cat("\nUnique communities (id_num):\n")
print(length(unique(ps2$id_num)))

cat("\nUnique states:\n")
print(length(unique(ps2$state)))

cat("\nCheck hityear coding (should be 0/1 only):\n")
print(table(ps2$hityear, useNA = "ifany"))

# Confirm panel uniqueness at the id-year level (should be exactly one observation per pair)

dup_id_year <- ps2 %>%
count(id_num, year) %>%
filter(n > 1)

stopifnot(nrow(dup_id_year) == 0)

```


## Step B.2: Construct each community‚Äôs flood year and merge it back to the panel

This chunk creates a community-level flood_year (the year with hityear == 1, if any) and merges it back into the full panel dataset.

```{r}

# Construct the flood year for each community (NA for never-flooded communities)

flood_year_by_id <- ps2 %>%
group_by(id_num) %>%
summarise(
flood_year = ifelse(any(hityear == 1, na.rm = TRUE),
year[which(hityear == 1)[1]],
NA_integer_),
.groups = "drop"
)

# Merge back to the main panel

ps2 <- ps2 %>%
left_join(flood_year_by_id, by = "id_num")

# Sanity checks

cat("Communities with a flood year:\n")
print(sum(!is.na(ps2$flood_year)) / length(ps2$flood_year))  # share at observation level (not id-level)

cat("\nUnique flood years (treated communities only):\n")
print(sort(unique(ps2$flood_year[!is.na(ps2$flood_year)])))

# Check floods-per-community is still at most one (assignment design)

floods_per_id <- ps2 %>%
group_by(id_num) %>%
summarise(total_floods = sum(hityear == 1, na.rm = TRUE), .groups = "drop")

cat("\nDistribution of floods per community (should be 0 or 1 only):\n")
print(table(floods_per_id$total_floods, useNA = "ifany"))

stopifnot(all(floods_per_id$total_floods %in% c(0, 1)))

```


## Step B.3: Construct treatment status and relative event time

This chunk creates a treated indicator and the relative year variableùëü= year ‚àí flood_year for treated communities, plus quick checks of the resulting event-time support.

```{r}

# Treated indicator and relative event time

ps2 <- ps2 %>%
mutate(
treated  = as.integer(!is.na(flood_year)),
rel_year = ifelse(treated == 1, year - flood_year, NA_integer_)
)

# Sanity checks at the community level (not observation level)

treated_share_ids <- ps2 %>%
distinct(id_num, flood_year) %>%
summarise(treated_share = mean(!is.na(flood_year))) %>%
pull(treated_share)

cat("Share of communities treated (should be 3092 / 7019):\n")
print(treated_share_ids)

cat("\nCheck treated indicator (observation-level counts):\n")
print(table(ps2$treated, useNA = "ifany"))

# Relative year support for treated observations only

cat("\nRelative year summary (treated observations only):\n")
print(summary(ps2$rel_year[ps2$treated == 1]))

cat("\nRelative year frequency (treated observations only; first few):\n")
print(head(sort(table(ps2$rel_year[ps2$treated == 1])), 15))

```


## Step b.4: Choose event window and bin/cap relative time

This chunk defines the event-study window, bins extreme leads and lags, and creates a factor variable with the correct reference period (r = ‚àí1). 

```{r}

# Choose event-study window

LOWER <- -6
UPPER <-  6

ps2 <- ps2 %>%
mutate(
rel_year_bin = case_when(
treated == 0              ~ NA_integer_,   # never-treated: no event time
rel_year <= LOWER         ~ LOWER,          # bin lower tail
rel_year >= UPPER         ~ UPPER,          # bin upper tail
TRUE                      ~ rel_year
)
)

# Check distribution after binning (treated only)

cat("Relative year (binned) distribution, treated observations only:\n")
print(sort(table(ps2$rel_year_bin[ps2$treated == 1]), decreasing = TRUE))

# Create ordered factor for regression and plotting

event_levels <- sort(unique(ps2$rel_year_bin[ps2$treated == 1]))
ps2$rel_year_f <- factor(ps2$rel_year_bin, levels = event_levels)

# Set reference period: r = -1

ps2$rel_year_f <- relevel(ps2$rel_year_f, ref = "-1")

cat("\nEvent-time factor levels (in order):\n")
print(levels(ps2$rel_year_f))

# Sanity check: confirm -1 is the reference

cat("\nReference level:\n")
print(levels(ps2$rel_year_f)[1] == "-1")

```


## Step B.5: Estimate the event-study regression (Gallagher-style)

This chunk runs the TWFE event-study regression with:
  - Outcome: ln_policies
  - Event-time indicators: rel_year_f
  - Fixed effects: community FE and state-by-year FE
  - Clustering: by state (to match Gallagher)

```{r}

# Event-study regression: Gallagher-style specification

es_gallagher <- fixest::feols(
ln_policies ~ i(rel_year_f, treated, ref = "-1") |
id_num + state^year,
data    = ps2,
cluster = ~ state
)

# Print regression summary (focus on event-time coefficients)

summary(es_gallagher)

# Extract only the event-time coefficients for inspection

es_coefs <- broom::tidy(es_gallagher) %>%
filter(grepl("rel_year_f", term))

cat("\nEvent-time coefficients (relative to r = -1):\n")
print(es_coefs)

```


## Step B.6: Build and save the event-study plot

This chunk extracts the event-time coefficients from the regression, constructs confidence intervals, and plots the dynamic treatment effects relative to the year before the flood.

```{r}

# Tidy event-time coefficients from the Gallagher-style regression
es_plot_data <- broom::tidy(es_gallagher) %>%
  filter(grepl("rel_year_f", term)) %>%
  mutate(
    rel_year = as.integer(stringr::str_extract(term, "-?\\d+")),
    ci_low   = estimate - 1.96 * std.error,
    ci_high  = estimate + 1.96 * std.error
  ) %>%
  arrange(rel_year) %>%
  # Restrict to the defensible window: -6 to +9
  filter(rel_year >= -6, rel_year <= 9)

# Print for a quick check
cat("Event-study plotting data (restricted to -6 to +9):\n")
print(es_plot_data)

# Build the event-study plot (serif = Times New Roman on Windows)
es_plot <- ggplot(es_plot_data, aes(x = rel_year, y = estimate)) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dotted") +
  geom_point() +
  geom_errorbar(aes(ymin = ci_low, ymax = ci_high), width = 0.2) +
  scale_x_continuous(breaks = seq(-6, 9, by = 1)) +
  labs(
    x = "Years relative to flood",
    y = "Effect on log policies (relative to year before flood)",
    title = "Event Study: Flood and Insurance Take-Up"
  ) +
  theme_minimal(base_family = "serif") +
  theme(
    plot.title = element_text(face = "bold"),
    axis.title = element_text(face = "bold")
  )

# Display the plot
print(es_plot)

# Save the plot
plot_file <- file.path(FIG_DIR, "event_study_gallagher_style_serif_m6_p9.png")
ggsave(plot_file, es_plot, width = 7, height = 5, dpi = 300)

cat("Event-study figure saved to:\n", plot_file, "\n")

```

## Part b) i) Interpretation of the Event-Study Results

Figure B.6 shows the dynamic effect of a flood on log flood‚Äêinsurance take-up, normalized to zero in the year before the flood. Pre-treatment coefficients are generally small and statistically indistinguishable from zero, indicating little evidence of strong differential pre-trends once community fixed effects and state-by-year fixed effects are included. One lead (three years before the flood) is slightly negative and marginally significant, but there is no consistent pattern of divergence prior to treatment, supporting the identifying assumption that - conditional on fixed effects - the timing of floods is plausibly exogenous.

In contrast, the post-flood response is large and immediate. Insurance take-up rises by about 9 log points (roughly 9 percent) in the flood year and peaks around 12 log points in the following year. The effect then gradually declines but remains positive and statistically significant for several years, before becoming imprecisely estimated by about six years after the flood. This spike-and-decay pattern closely matches the dynamics emphasized in Gallagher: households strongly update beliefs after a recent flood, but the effect fades as the event becomes less salient, consistent with limited memory or overweighting of recent experience rather than full Bayesian learning.

## Part b) ii) Clustering of Standard Errors

Standard errors in the event-study regression are clustered at the state level, following Gallagher (2014). This choice is motivated by the likelihood that shocks affecting flood insurance demand‚Äîsuch as regulatory changes, insurance market conditions, or media coverage‚Äîare correlated across communities within the same state over time. Because treatment varies at the community level but key unobservables may be correlated at higher geographic levels, failing to account for this intra-state correlation would understate uncertainty and lead to over-rejection. Given that the specification also includes state-by-year fixed effects, clustering at the state level is a natural and coherent choice: it allows for arbitrary serial and cross-sectional correlation in the error term within states, while still exploiting within-state variation in flood timing across communities for identification.

Because Gallagher emphasizes that beliefs are shaped through shared TV news exposure, shocks to insurance demand are likely correlated within local media markets (DMAs), which often cross state borders. This suggests that state-level clustering may understate uncertainty if media markets are the main channel of information transmission. A more conservative alternative would be to cluster at the media-market level, or to use two-way clustering by state and media market to allow for correlation along both regulatory and information channels.

## Judgement Calls

Event-time indicators are capped at [‚àí6, +6], with more extreme leads and lags binned into endpoint categories. This follows standard practice in event-study designs and Gallagher (2014), since extreme event times are identified off few observations and produce noisy and unstable estimates. Communities that experience floods near the beginning or end of the sample therefore contribute to the binned endpoint categories rather than being dropped entirely. Communities that never experience a flood are retained in the sample and contribute to the estimation of fixed effects, but do not load on any event-time indicators, so they serve as additional controls in the two-way fixed effects framework.


## Part (c): Diagnosing TWFE under staggered treatment using the Goodman-Bacon decomposition

This section evaluates the potential shortcomings of the two-way fixed effects (TWFE) approach in a staggered-adoption setting and quantifies the contribution of different 2√ó2 comparisons using the Goodman-Bacon decomposition. The workflow proceeds by (i) constructing treatment timing/cohorts and the post-treatment indicator used in TWFE, (ii) estimating the baseline TWFE DiD effect, and (iii) decomposing that estimate into its constituent comparison types to assess the role of ‚Äúlate-to-early‚Äù comparisons.

## Step C.1: Construct treatment timing, cohorts, and the TWFE treatment indicator

This chunk constructs each community‚Äôs flood year (treatment timing), defines treatment cohorts, and creates the post-treatment indicator used in the TWFE DiD regression and in the Goodman-Bacon decomposition.

```{r}

# Treatment timing, cohorts, and TWFE treatment indicator (for Part c)
# Start from the raw panel loaded in Step 0.5

ps2 <- ps2_raw

# Identify the flood year for each treated community (at most one flood per id_num by design)

treat_timing <- ps2 %>%
filter(hityear == 1) %>%
select(id_num, year) %>%
rename(flood_year = year)

# Merge flood_year back into the full panel

ps2_c <- ps2 %>%
left_join(treat_timing, by = "id_num")

# Construct cohort and post-treatment indicator
# - cohort_year: flood year for treated units, NA for never-treated units
# - ever_treated: indicator for ever-treated units (ever flooded)
# - post: indicator for being in post-treatment periods (including flood year)

ps2_c <- ps2_c %>%
mutate(
cohort_year  = flood_year,
ever_treated = as.integer(!is.na(flood_year)),
post         = as.integer(ever_treated == 1 & year >= flood_year)
)

# Sanity checks for timing objects

cat("Cohort / timing checks:\n")
cat("Total observations:\n"); print(nrow(ps2_c))

cat("\nUnique treated communities:\n")
print(length(unique(ps2_c$id_num[ps2_c$ever_treated == 1])))

cat("Unique never-treated communities:\n")
print(length(unique(ps2_c$id_num[ps2_c$ever_treated == 0])))

cat("\nFlood year summary (treated only):\n")
print(summary(ps2_c$flood_year))

cat("\nCohort counts (treated only):\n")
print(
ps2_c %>%
filter(ever_treated == 1) %>%
distinct(id_num, cohort_year) %>%
count(cohort_year) %>%
arrange(cohort_year)
)

# Verify post indicator behaves as expected for treated units

cat("\nPost-treatment indicator check (treated units only):\n")
print(
ps2_c %>%
filter(ever_treated == 1) %>%
summarise(
min_year   = min(year),
max_year   = max(year),
min_post   = min(post, na.rm = TRUE),
max_post   = max(post, na.rm = TRUE),
share_post = mean(post, na.rm = TRUE)
)
)

```


## Step C.2: Estimate the baseline TWFE DiD effect

This chunk estimates the standard two-way fixed effects difference-in-differences model using the post-treatment indicator. The resulting coefficient is the single TWFE estimate that will later be diagnosed using the Goodman-Bacon decomposition.

```{r}

# Baseline TWFE DiD regression
# TWFE specification:
# ln_policies_it = beta * post_it + community FE + year FE + error_it
# Clustering at the state level, consistent with Part (b)

twfe_mod <- feols(
ln_policies ~ post | id_num + year,
data = ps2_c,
cluster = ~state
)

# Display results

summary(twfe_mod)

# Store coefficient of interest for later comparison

twfe_beta <- coef(twfe_mod)["post"]
cat("\nTWFE DiD estimate (beta on post):\n")
print(twfe_beta)

```


## Step C.3: Goodman-Bacon decomposition of the TWFE estimate

This chunk applies the Goodman-Bacon decomposition to the TWFE DiD estimate in order to decompose it into all underlying 2√ó2 comparisons (treated vs never-treated, early vs late, and late vs early) and to record the weight and contribution of each comparison type.

```{r}

## Step C.3: Goodman‚ÄìBacon decomposition of the TWFE estimate (bacondecomp)
# This chunk runs the Goodman‚ÄìBacon decomposition of the TWFE regression ln_policies ~ post.
# The output is saved to disk so the decomposition is computed only once.

library(bacondecomp)

# Minimal, base-R input required by bacondecomp::bacon()
# - Keep only variables needed for the decomposition
# - Coerce to primitive types
# - Ensure the object is a plain data.frame (not tibble)
ps2_bacon <- ps2_c[, c("id_num", "year", "ln_policies", "post")]
ps2_bacon <- data.frame(
  id_num      = as.integer(ps2_bacon$id_num),
  year        = as.integer(ps2_bacon$year),
  ln_policies = as.numeric(ps2_bacon$ln_policies),
  post        = as.integer(ps2_bacon$post)
)

# Verify balanced panel (required by bacondecomp::bacon)
nT_by_id <- table(ps2_bacon$id_num)
if (length(unique(nT_by_id)) != 1) {
  stop("Unbalanced panel detected: not all id_num have the same number of time periods.")
}

# Verify no missing values in required columns (required by bacondecomp::bacon)
if (anyNA(ps2_bacon)) {
  stop("Missing values detected in id_num/year/ln_policies/post; bacondecomp::bacon() requires no NAs.")
}

# Verify treatment is binary
if (!all(ps2_bacon$post %in% c(0L, 1L))) {
  stop("post is not binary (0/1).")
}

# Run the decomposition (can be slow; run once and save results)
cat("Running bacondecomp::bacon() decomposition...\n")
bacon_time <- system.time({
  bacon_tbl <- bacondecomp::bacon(
    formula  = ln_policies ~ post,
    data     = ps2_bacon,
    id_var   = "id_num",
    time_var = "year",
    quietly  = TRUE
  )
})
cat("bacon() runtime (seconds):\n")
print(bacon_time)

# bacon_tbl is already a data.frame of 2x2 comparisons in the no-controls case
# Add contribution for later reporting
bacon_tbl$contribution <- bacon_tbl$estimate * bacon_tbl$weight

# Save results so the decomposition never has to be recomputed
bacon_outfile <- file.path(OUTPUT_DIR, "bacon_decomposition.rds")
saveRDS(bacon_tbl, bacon_outfile)
cat("Decomposition saved to:\n", bacon_outfile, "\n")

# Basic summaries needed for Part (c)
cat("\nTotal weight by comparison type:\n")
print(aggregate(weight ~ type, data = bacon_tbl, sum))

cat("\nWeighted average of component estimates (diagnostic):\n")
weighted_avg <- sum(bacon_tbl$estimate * bacon_tbl$weight)
print(weighted_avg)

cat("\nTWFE estimate from Step C.2 (beta on post):\n")
print(twfe_beta)

```

The Goodman‚ÄìBacon decomposition successfully decomposes the TWFE DiD estimate into its underlying 2√ó2 comparisons. The diagnostic check confirms internal consistency: the weighted average of all component estimates exactly reproduces the TWFE coefficient of 0.1211. Most identifying variation (about 71.7%) comes from comparisons between treated and never-treated communities, which are conceptually clean and align with the canonical DiD design. However, a nontrivial share of weight (about 26%) comes from comparisons among treated units at different adoption times.

In particular, approximately 10.0% of the total weight is placed on ‚ÄúLater vs Earlier Treated‚Äù comparisons, in which already-treated units serve as controls for later-treated units. These comparisons are precisely those emphasized in the recent staggered-adoption literature as potentially problematic when treatment effects are dynamic or heterogeneous. The presence of meaningful weight on these comparisons implies that the TWFE estimate is not solely driven by treated-versus-untreated contrasts, and may therefore partly reflect comparisons that use previously treated units as controls.


## Step C.4: Summarize the decomposition and quantify late-to-early comparisons

This chunk loads the saved Goodman‚ÄìBacon decomposition output, summarizes weights and implied estimates by comparison type, and isolates the contribution of ‚ÄúLater vs Earlier Treated‚Äù comparisons.

```{r}

# Step C.4: Summarize Goodman‚ÄìBacon decomposition output

# Load the saved decomposition table from Step C.3

bacon_infile <- file.path(OUTPUT_DIR, "bacon_decomposition.rds")

if (!file.exists(bacon_infile)) {
stop("Saved decomposition not found. Run Step C.3 to create bacon_decomposition.rds first.")
}

bacon_tbl <- readRDS(bacon_infile)

# Basic checks

cat("Decomposition table loaded.\n")
cat("Rows in decomposition table:\n"); print(nrow(bacon_tbl))

cat("\nTotal weight (should equal 1):\n")
print(sum(bacon_tbl$weight, na.rm = TRUE))

# Ensure contribution exists (older saved files may not include it)

if (!("contribution" %in% names(bacon_tbl))) {
bacon_tbl$contribution <- bacon_tbl$estimate * bacon_tbl$weight
}

# Summarize by comparison type:

# - total_weight: total weight placed on that comparison type

# - implied_estimate: weighted average estimate within that type

# - total_contribution: sum(weight * estimate) = contribution to overall TWFE

bacon_by_type <- bacon_tbl %>%
group_by(type) %>%
summarise(
total_weight       = sum(weight, na.rm = TRUE),
implied_estimate   = sum(estimate * weight, na.rm = TRUE) / sum(weight, na.rm = TRUE),
total_contribution = sum(contribution, na.rm = TRUE),
.groups = "drop"
) %>%
arrange(desc(total_weight))

cat("\nSummary by comparison type:\n")
print(bacon_by_type)

# Pull out key types for emphasis

treated_vs_untreated <- bacon_by_type %>% filter(type == "Treated vs Untreated")
early_vs_late        <- bacon_by_type %>% filter(type == "Earlier vs Later Treated")
late_vs_early        <- bacon_by_type %>% filter(type == "Later vs Earlier Treated")
late_vs_always       <- bacon_by_type %>% filter(type == "Later vs Always Treated")

cat("\nKey blocks (weight, implied estimate, contribution):\n")
cat("\nTreated vs Untreated:\n"); print(treated_vs_untreated)
cat("\nEarlier vs Later Treated:\n"); print(early_vs_late)
cat("\nLater vs Earlier Treated (late-to-early):\n"); print(late_vs_early)
cat("\nLater vs Always Treated:\n"); print(late_vs_always)

# Verify that contributions sum to the TWFE estimate

cat("\nSum of contributions across all types (should equal TWFE beta):\n")
print(sum(bacon_by_type$total_contribution, na.rm = TRUE))

cat("\nTWFE estimate from Step C.2 (beta on post):\n")
print(twfe_beta)

# Compute shares of the TWFE estimate attributable to each comparison type

# (This is contribution divided by overall TWFE; can be unstable if twfe_beta is near zero.)

if (!is.na(twfe_beta) && abs(twfe_beta) > 1e-8) {
bacon_by_type <- bacon_by_type %>%
mutate(share_of_twfe = total_contribution / twfe_beta)

cat("\nShare of TWFE estimate attributable to each type (contribution / TWFE beta):\n")
print(bacon_by_type %>% select(type, total_weight, total_contribution, share_of_twfe))
} else {
cat("\nNOTE: TWFE beta is near zero; shares (contribution / beta) are not reported.\n")
}

# Clean rounded table for the write-up

bacon_by_type_clean <- bacon_by_type %>%
mutate(
total_weight       = round(total_weight, 6),
implied_estimate   = round(implied_estimate, 6),
total_contribution = round(total_contribution, 6),
share_of_twfe      = if ("share_of_twfe" %in% names(bacon_by_type)) round(share_of_twfe, 6) else NA_real_
)

cat("\nRounded summary table (for write-up):\n")
print(bacon_by_type_clean)

# Save summary table to output/tables

bacon_summary_outfile <- file.path(TAB_DIR, "bacon_summary_by_type.csv")
write.csv(bacon_by_type_clean, bacon_summary_outfile, row.names = FALSE)
cat("\nSummary table saved to:\n", bacon_summary_outfile, "\n")

```

Step C.4 adds a quantitative layer to the decomposition by showing that the potentially problematic ‚Äúlate-to-early‚Äù comparisons not only receive nontrivial weight (about 10%), but also imply substantially larger effects (0.210) than the clean treated-versus-untreated comparisons (0.115). This pattern is consistent with dynamic treatment effects: in the flood-insurance context, communities that experienced floods earlier have had more time to update beliefs and increase insurance take-up, so they appear much more responsive when later-treated communities are used as a comparison group. As emphasized in the staggered-adoption literature, such dynamics make treated-as-control comparisons particularly misleading. Here, they account for about 17% of the overall TWFE estimate and push it upward, suggesting that the TWFE coefficient partly reflects dynamic learning effects being mis-weighted across cohorts rather than a single, stable average treatment effect.


## Step C.5: Final synthesis of TWFE vs Goodman‚ÄìBacon components

This chunk constructs a concise summary table that contrasts the overall TWFE estimate with its main Goodman‚ÄìBacon components, showing how different treatment-timing comparisons are weighted and how they shape (or distort) the aggregate TWFE estimate.

```{r}

# This chunk prints a complete decomposition table (all comparison types), with weights summing to 1 and contributions summing to the TWFE estimate.

library(knitr)
library(kableExtra)
library(dplyr)

# Load the Step C.4 summary table (by comparison type)
bacon_summary_file <- file.path(TAB_DIR, "bacon_summary_by_type.csv")

if (!file.exists(bacon_summary_file)) {
  stop("Bacon summary file not found. Run Step C.4 first.")
}

bacon_by_type <- read.csv(bacon_summary_file, stringsAsFactors = FALSE)

# Keep only the expected columns (prevents duplicate/NA columns from accidental binds)
needed_cols <- c("type", "total_weight", "implied_estimate", "total_contribution")
if (!all(needed_cols %in% names(bacon_by_type))) {
  stop("bacon_summary_by_type.csv does not contain the expected columns: type, total_weight, implied_estimate, total_contribution.")
}
bacon_by_type <- bacon_by_type[, needed_cols]

# Order the comparison types in a conventional way
type_order <- c(
  "Treated vs Untreated",
  "Earlier vs Later Treated",
  "Later vs Earlier Treated",
  "Later vs Always Treated"
)

bacon_full_tbl <- bacon_by_type %>%
  mutate(type = factor(type, levels = type_order)) %>%
  arrange(type) %>%
  transmute(
    `Comparison type`       = as.character(type),
    `Implied 2x2 estimate`  = as.numeric(implied_estimate),
    `Weight`                = as.numeric(total_weight),
    `Contribution`          = as.numeric(total_contribution)
  )

# Add a "Total" row (weights sum to 1; contribution equals TWFE beta)
total_row <- data.frame(
  `Comparison type`      = "Total",
  `Implied 2x2 estimate` = NA_real_,
  `Weight`               = sum(bacon_full_tbl$Weight, na.rm = TRUE),
  `Contribution`         = sum(bacon_full_tbl$Contribution, na.rm = TRUE),
  check.names = FALSE
)

# Add a TWFE reference row (weight is not a decomposition weight, so leave as NA)
twfe_row <- data.frame(
  `Comparison type`      = "TWFE (overall)",
  `Implied 2x2 estimate` = as.numeric(twfe_beta),
  `Weight`               = NA_real_,
  `Contribution`         = as.numeric(twfe_beta),
  check.names = FALSE
)

bacon_full_tbl <- bind_rows(twfe_row, bacon_full_tbl, total_row)

# Format numbers for display (remove NA text; show em dash instead)
fmt_num <- function(x, digits = 4) {
  ifelse(is.na(x), "\u2014", sprintf(paste0("%.", digits, "f"), x))
}

bacon_full_tbl_pretty <- bacon_full_tbl %>%
  mutate(
    `Implied 2x2 estimate` = fmt_num(`Implied 2x2 estimate`, 4),
    `Weight`               = fmt_num(`Weight`, 4),
    `Contribution`         = fmt_num(`Contribution`, 4)
  )

# Save a clean CSV (numeric) for reproducibility / appendix use
bacon_full_outfile <- file.path(TAB_DIR, "bacon_full_table.csv")
write.csv(bacon_full_tbl, bacon_full_outfile, row.names = FALSE)
cat("\nFull Bacon decomposition table saved to:\n", bacon_full_outfile, "\n")

# Print publication-ready table in the knitted report
kable(
  bacon_full_tbl_pretty,
  caption  = "Goodman‚ÄìBacon Decomposition of the TWFE Estimate",
  align    = c("l", "c", "c", "c"),
  booktabs = TRUE
) %>%
  kable_styling(
    font_size  = 11,
    position   = "center",
    full_width = FALSE,
    latex_options = c("hold_position")
  ) %>%
  row_spec(0, bold = TRUE) %>%
  # Bold the TWFE and Total rows
  row_spec(which(bacon_full_tbl$`Comparison type` %in% c("TWFE (overall)", "Total")), bold = TRUE) %>%
  # Emphasize late-to-early comparisons
  row_spec(which(bacon_full_tbl$`Comparison type` == "Later vs Earlier Treated"), bold = TRUE) %>%
  column_spec(1, width = "7cm") %>%
  kable_classic(full_width = FALSE, html_font = "Times New Roman")

```

## Final Interpretation of Part c)

Recent work on staggered difference-in-differences designs shows that two-way fixed effects (TWFE) and event-study estimators can produce misleading estimates when treatment timing is staggered and treatment effects are dynamic. As emphasized by Baker et al. (2022), the TWFE estimand is generally a weighted average of heterogeneous 2√ó2 comparisons, including comparisons that use already-treated units as controls for later-treated units. When treatment effects evolve over time‚Äîas is plausible in Gallagher‚Äôs (2014) setting, where households update beliefs about flood risk after experiencing a flood‚Äîsuch ‚Äútreated-as-control‚Äù comparisons need not recover a meaningful average treatment effect on the treated (ATT).

The Goodman‚ÄìBacon decomposition reported in Table X shows that about 71.7% of the identifying weight in the TWFE estimate comes from clean comparisons between treated and never-treated communities, which imply an average effect of 0.1147. However, a nontrivial share of weight is placed on comparisons among treated cohorts. In particular, ‚Äúlate-to-early‚Äù comparisons‚Äîwhere later-treated units are compared to already-treated units‚Äîreceive about 9.98% of the total weight and imply an ATT of 0.2102 (Table X). This estimate is substantially larger than the effect implied by treated-versus-untreated comparisons, consistent with the presence of dynamic treatment effects that grow with time since treatment.

As shown in Table X, late-to-early comparisons contribute approximately 0.0210 to the overall TWFE estimate of 0.1211, accounting for roughly 17% of the aggregate effect. Thus, a meaningful portion of the TWFE estimate is driven by treated-as-control comparisons that are precisely those highlighted in the recent literature as potentially problematic under staggered adoption with dynamic effects. In the context of flood-insurance take-up, this suggests that Gallagher‚Äôs TWFE/event-study estimates partly reflect dynamic learning effects being re-weighted across cohorts, rather than a single stable ATT, and should therefore be interpreted with caution.


## Part (d): Addressing staggered DiD bias using the Borusyak et al. imputation estimator

This section implements the imputation estimator of Borusyak et al. (2024) to resolve the identification problems of TWFE under staggered treatment timing documented in Part (c). The approach proceeds by (i) estimating unit and time fixed effects using only untreated observations, (ii) imputing untreated potential outcomes for treated observations and constructing observation-level treatment effects, and (iii) using these imputed effects to build an event-study that traces the dynamic impact of flood experience without relying on already-treated units as controls.

## Step D.1: Construct flood timing, treatment status, and imputation sample indicators

This chunk constructs the unit-level flood timing variable (flood_year) from the flood-year indicator (hityear), then defines the time-varying treatment indicator (treated_it) and the untreated/treated observation flags (‚Ñ¶‚ÇÄ and ‚Ñ¶‚ÇÅ) used in Steps (1)‚Äì(3).

```{r}

# Construct unit-level flood timing (flood_year) and then define treated_it and Œ©0/Œ©1 at the observation level

ps2 <- ps2 %>%
group_by(id_num) %>%
mutate(
# Flood year: the (unique) year in which hityear == 1 for the community; NA if never flooded
flood_year = ifelse(any(hityear == 1L), year[hityear == 1L][1], NA_integer_),

# Time-varying treatment indicator: treated in and after the flood year
treated_it = ifelse(!is.na(flood_year) & year >= flood_year, 1L, 0L),

# Unit ever treated (has a flood at some point in the sample)
ever_treated = ifelse(!is.na(flood_year), 1L, 0L),

# Œ©0: untreated observations (never-treated units and pre-treatment periods)
in_omega0 = ifelse(treated_it == 0L, 1L, 0L),

# Œ©1: treated observations (post-treatment periods for treated units)
in_omega1 = ifelse(treated_it == 1L, 1L, 0L)

) %>%
ungroup()

# Explicit sanity checks

cat("Sanity check A: Flood events per community (should be 0 or 1 only)\n")
flood_counts <- ps2 %>%
group_by(id_num) %>%
summarise(n_flood_years = sum(hityear == 1L), .groups = "drop")
print(table(flood_counts$n_flood_years))

cat("\nSanity check B: flood_year missingness (NA means never flooded)\n")
print(table(is.na(ps2$flood_year)))

cat("\nSanity check C: Every observation classified as either Œ©0 or Œ©1 (should be all 1s)\n")
print(table(ps2$in_omega0 + ps2$in_omega1))

cat("\nSanity check D: treated_it takes only values 0 or 1\n")
print(table(ps2$treated_it))

cat("\nSanity check E: treated_it equals Œ©1 by construction (cross-tab)\n")
print(table(treated_it = ps2$treated_it, in_omega1 = ps2$in_omega1))

```


## Step D.2: Estimate fixed effects using only untreated observations

This chunk estimates the unit and time fixed effects using only untreated observations (‚Ñ¶‚ÇÄ), which will later be used to impute untreated potential outcomes for treated observations.

```{r}

# Estimate unit and time fixed effects using only untreated observations (Œ©0)

fe_untreated <- feols(
ln_policies ~ 1 | id_num + year,
data = ps2 %>% filter(in_omega0 == 1)
)

# Extract estimated fixed effects

lambda_hat <- fixef(fe_untreated)$id_num
gamma_hat  <- fixef(fe_untreated)$year

# Merge fixed effects back into the main dataset

ps2 <- ps2 %>%
mutate(
lambda_hat = lambda_hat[as.character(id_num)],
gamma_hat  = gamma_hat[as.character(year)]
)

# Basic check

cat("Fixed effects successfully merged:\n")
cat("Missing lambda_hat:", sum(is.na(ps2$lambda_hat)), "\n")
cat("Missing gamma_hat:", sum(is.na(ps2$gamma_hat)), "\n")

```


## Step D.3: Impute untreated potential outcomes and treatment effects

This chunk constructs the imputed untreated potential outcome Y0_hat_it = lambda_hat_i + gamma_hat_t, and then computes the imputed treatment effect tau_hat_it = Y_it ‚àí Y0_hat_it.

```{r}

# Construct imputed untreated potential outcomes and treatment effects

ps2 <- ps2 %>%
mutate(
y0_hat = lambda_hat + gamma_hat,
tau_hat = ln_policies - y0_hat
)

# Keep treated observations for treatment-effect analysis

tau_data <- ps2 %>% filter(in_omega1 == 1)

# Sanity checks

cat("Sanity check A: Missing imputed values among treated observations\n")
cat("Missing y0_hat:", sum(is.na(tau_data$y0_hat)), "\n")
cat("Missing tau_hat:", sum(is.na(tau_data$tau_hat)), "\n")

cat("\nSanity check B: Summary of imputed treatment effects (treated obs only)\n")
print(summary(tau_data$tau_hat))

```


## Step D.4: Construct event time for treated observations

This chunk constructs relative (event) time for treated observations, which will be used to build the imputation-based event-study.

```{r}

# Construct event time for treated observations

tau_data <- tau_data %>%
mutate(
rel_year = year - flood_year
)

# Sanity checks for event time

cat("Sanity check A: Distribution of relative years (treated obs only)\n")
print(summary(tau_data$rel_year))

cat("\nSanity check B: Frequency table of relative years (first few values)\n")
print(head(sort(table(tau_data$rel_year)), 10))

```


## Step D.5: Imputation-based event-study regression

This chunk estimates the dynamic treatment effects by regressing the imputed treatment effects (tau_hat) on relative-time indicators, normalizing event time ‚àí1 as the reference period (implemented by including ‚Ñ¶‚ÇÄ in the regression with tau_hat = 0 for untreated observations).

```{r}

# Prepare event-study dataset: set tau_hat = 0 for Œ©0 and keep imputed tau_hat for Œ©1

es_data <- ps2 %>%
mutate(
rel_year = year - flood_year,
tau_es = ifelse(in_omega1 == 1, tau_hat, 0)
) %>%
filter(!is.na(rel_year)) %>%      # drops never-treated units (no event time)
filter(!is.na(tau_es))            # drops units with missing imputation (no lambda_hat)

# Run imputation-based event study (normalize to rel_year = -1)

es_imp <- feols(
tau_es ~ i(rel_year, ref = -1),
data = es_data,
cluster = ~ id_num
)

# Quick output

cat("Imputation-based event-study regression complete.\n")
print(summary(es_imp))


```


Step D.6: Publication-ready imputation event-study plot (‚àí6 to +9 window)

This chunk produces a report-ready imputation-based event-study figure, restricting the displayed event-time window to ‚àí6 through +9 for a clean and defensible presentation.

```{r}

# Tidy event-time coefficients from the imputation event-study regression

es_imp_plot_data <- broom::tidy(es_imp) %>%
filter(grepl("^rel_year::", term)) %>%
mutate(
rel_year = as.integer(gsub("rel_year::", "", term)),
ci_low   = estimate - 1.96 * std.error,
ci_high  = estimate + 1.96 * std.error
) %>%
arrange(rel_year) %>%

# Restrict to the defensible window: -6 to +9

filter(rel_year >= -6, rel_year <= 9)

# Print for a quick check

cat("Imputation event-study plotting data (restricted to -6 to +9):\n")
print(es_imp_plot_data)

# Build the publication-ready plot (serif = Times-like)

library(ggplot2)

es_imp_plot <- ggplot(es_imp_plot_data, aes(x = rel_year, y = estimate)) +
geom_hline(yintercept = 0, linetype = "dashed") +
geom_vline(xintercept = 0, linetype = "dotted") +
geom_point(size = 2) +
geom_errorbar(aes(ymin = ci_low, ymax = ci_high), width = 0.2) +
scale_x_continuous(breaks = seq(-6, 9, by = 1)) +
labs(
x = "Years relative to flood",
y = "Imputed treatment effect on log policies",
title = "Dynamic Effects of Flood Experience on Insurance Take-Up",
subtitle = "Imputation-based event study (Borusyak et al. method)"
) +
theme_minimal(base_family = "serif") +
theme(
plot.title = element_text(face = "bold"),
axis.title = element_text(face = "bold"),
panel.grid.minor = element_blank()
)

# Display the plot

print(es_imp_plot)

# Save high-resolution version for report

plot_file <- file.path(FIG_DIR, "event_study_imputation_serif_m6_p9.png")
ggsave(plot_file, es_imp_plot, width = 7, height = 5, dpi = 300)

cat("Imputation event-study figure saved to:\n", plot_file, "\n")

```

## PArt d) final interpretation

The imputation estimator of Borusyak et al. (2024) is implemented to address the biases that arise in two-way fixed effects models with staggered treatment timing. Using only untreated observations (never-treated units and pre-flood periods of treated units), a model of the form
Y_it = lambda_i + gamma_t + error_it
is first estimated. These estimates are then used to impute untreated potential outcomes for treated observations as
Y0_hat_it = lambda_hat_i + gamma_hat_t,
and treatment effects are constructed as
tau_hat_it = Y_it ‚àí Y0_hat_it.
This procedure ensures that treatment effects are identified only from comparisons with untreated observations, thereby avoiding the ‚Äúalready-treated as control‚Äù problem discussed in Part (c).

Figure Y reports the event-study coefficients obtained by regressing the imputed treatment effects on relative-time indicators. The figure shows a sharp and statistically significant increase in insurance take-up in the flood year, followed by a peak in the first one to two years after the flood. The effect then declines gradually over subsequent years and approaches zero at longer horizons. Pre-treatment coefficients are essentially zero, reflecting the construction of the estimator and supporting the absence of differential pre-trends.

Compared with the standard TWFE event study, the imputation-based results in Figure Y display a similar qualitative pattern‚Äîan immediate spike followed by gradual decay‚Äîbut with a cleaner dynamic profile at longer horizons. By eliminating comparisons that use already-treated units as controls, the imputation estimator removes the problematic weighting documented in Part (c). The results therefore confirm Gallagher‚Äôs substantive conclusion that flood experience temporarily raises perceived risk, as proxied by insurance take-up, while demonstrating that this conclusion is robust to using an estimator designed for staggered treatment timing.


## Part (e): Comparing ATT estimates and inference under imputation

This section compares the average treatment effect on the treated (ATT) obtained from the ‚Äúby hand‚Äù imputation approach with the ATT produced by the did_imputation package. The workflow proceeds by (i) aggregating the imputed treatment effects to obtain the ATT using a simple regression/mean, (ii) estimating the same estimand using the package implementation of the imputation estimator, and (iii) contrasting the resulting point estimates and standard errors to illustrate why naive standard errors from the hand-constructed approach are incorrect and how proper inference must account for first-stage estimation uncertainty.

## Step E.1: Estimate ATT from the ‚Äúby hand‚Äù imputation approach (naive SE)

This chunk estimates the ATT implied by the hand-constructed imputation effects by regressing the imputation-based treatment effect variable on the treatment indicator; the coefficient equals the mean imputed effect among treated observations.

```{r}

# Build the analysis variable used for the ATT regression:

# tau_es = tau_hat for treated observations, and 0 for untreated observations

att_data <- ps2 %>%
mutate(
tau_es = ifelse(in_omega1 == 1, tau_hat, 0)
) %>%

# drop observations where tau_hat is missing (these units never appear untreated in Œ©0)

filter(!is.na(tau_es))

# ATT via regression (equivalent to mean(tau_hat | treated))

att_hand <- feols(
tau_es ~ treated_it,
data = att_data,
cluster = ~ id_num
)

cat("Hand (by-construction) imputation ATT estimate:\n")
print(summary(att_hand))

# Cross-check: direct mean of tau_hat among treated observations (non-missing only)

att_mean <- att_data %>%
filter(treated_it == 1) %>%
summarise(att = mean(tau_es), n = n())

cat("\nCross-check (mean of tau_es among treated observations):\n")
print(att_mean)

```


## Step E.2: Estimate ATT using the didimputation package

This chunk estimates the ATT using the did_imputation() function from the didimputation package, using the same first-stage specification (unit and year fixed effects) and clustering at the community level.

```{r}

# Load package (install once if needed)

# install.packages("didimputation")

library(didimputation)

att_pkg <- did_imputation(
data = ps2,
yname = "ln_policies",
gname = "flood_year",                 # treatment start year (NA for never-treated is allowed)
tname = "year",
idname = "id_num",
first_stage = ~ 1 | id_num + year,    # match Step D.2
cluster_var = "id_num"                # default is idname, but kept explicit
)

cat("Package did_imputation ATT estimate:\n")
print(att_pkg)

```


## Step E.3: Professional ATT comparison table for the report

This chunk formats the ATT estimates and standard errors into a publication-ready table and saves it as both HTML and LaTeX (choose whichever matches your final output format).

```{r}

## Step E.3: Publication-ready ATT comparison table

library(knitr)
library(kableExtra)
library(dplyr)

# Build a clean numeric table first (safe column names)
att_tbl <- data.frame(
  Method    = c("Hand imputation (naive SE)", "did_imputation (correct SE)"),
  ATT       = c(as.numeric(hand_est), as.numeric(pkg_est)),
  Std_Error = c(as.numeric(hand_se),  as.numeric(pkg_se)),
  check.names = FALSE
)

# Format numbers for display
fmt_num <- function(x, digits = 4) sprintf(paste0("%.", digits, "f"), x)

att_tbl_pretty <- att_tbl %>%
  mutate(
    ATT       = fmt_num(ATT, 4),
    Std_Error = fmt_num(Std_Error, 4)
  ) %>%
  rename(`Std. Error` = Std_Error)

# Save a clean CSV (numeric) for reproducibility / appendix use
att_outfile <- file.path(TAB_DIR, "att_comparison_table.csv")
write.csv(att_tbl, att_outfile, row.names = FALSE)
cat("\nATT comparison table saved to:\n", att_outfile, "\n")

# Print publication-ready table in the knitted report (matching earlier style)
kable(
  att_tbl_pretty,
  caption  = "ATT comparison: hand imputation versus did_imputation",
  align    = c("l", "c", "c"),
  booktabs = TRUE
) %>%
  kable_styling(
    font_size  = 11,
    position   = "center",
    full_width = FALSE,
    latex_options = c("hold_position")
  ) %>%
  row_spec(0, bold = TRUE) %>%
  # Emphasize the package row (valid inference)
  row_spec(which(att_tbl$Method == "did_imputation (correct SE)"), bold = TRUE) %>%
  column_spec(1, width = "7cm") %>%
  kable_classic(full_width = FALSE, html_font = "Times New Roman")

```

## Part e) Final Interpretation

Table Z reports the average treatment effect on the treated (ATT) estimated using both the hand implementation of the imputation estimator and the did_imputation package. Both methods produce the same point estimate (ATT = 0.0917), which confirms that they implement the same imputation logic: untreated outcomes are first estimated using only untreated observations, and treatment effects are then constructed as tau_hat_it = Y_it ‚àí Y0_hat_it. This equivalence of point estimates is exactly what is predicted by the imputation framework of Borusyak et al. (2024) and by the exposition in Roth et al. (2023).

The difference appears in the standard errors. The hand implementation reports a smaller standard error (0.0134) because it treats the imputed components as known and ignores uncertainty from the first-stage estimation. The did_imputation package reports a larger standard error (0.0144) because it accounts for the imputation step and the resulting dependence structure, as required by Borusyak et al. (2024). Roth et al. (2023) warn that naive second-stage regressions on imputed effects lead to invalid inference, and Baker et al. (2022) emphasize that correcting staggered-adoption bias requires both correct point estimation and correct inference. Therefore, the package-based standard error is the appropriate one for inference.


```{r}


```




```{r}


```




```{r}


```




```{r}


```




```{r}


```




```{r}


```




```{r}


```




```{r}


```


